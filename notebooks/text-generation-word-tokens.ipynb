{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all the packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all the layer types that we gonna need\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text_filename = \"paul_graham.txt\"\n",
    "training_raw_text = open(\n",
    "    training_text_filename,\n",
    "    'r',\n",
    "    encoding='utf-8')\\\n",
    "    .read()\\\n",
    "    .lower()\\\n",
    "    .replace(\"\\r\",\" \")\\\n",
    "    .replace(\"\\n\",\" \")\\\n",
    "    .replace(\"\\t\",\" \")\\\n",
    "    .replace(\"\\\"\", \"\")\\\n",
    "    .replace(\"\\'\", \"\")\\\n",
    "    .replace(\"(\", \"\")\\\n",
    "    .replace(\")\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the data was actually read correctly\n",
    "training_raw_text = training_raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word tokens and mappings from token_id to token and token to token_id\n",
    "tokens = sorted(list(set(tokenize_string(training_raw_text))))\n",
    "token_to_id = dict((token, token_id) for token_id, token in enumerate(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17868\n"
     ]
    }
   ],
   "source": [
    "def tokenize_string(a_string: str) -> List[str]:\n",
    "    stop_words = open(\"stop_words.txt\", \"r\").read().split('\\n')\n",
    "    ret_list = []\n",
    "    build_word = \"\"\n",
    "    a_string = a_string\\\n",
    "        .lower()\\\n",
    "        .replace(\"\\r\",\" \")\\\n",
    "        .replace(\"\\n\",\" \")\\\n",
    "        .replace(\"\\t\",\" \")\\\n",
    "        .replace(\"\\\"\", \"\")\\\n",
    "        .replace(\"\\'\", \"\")\\\n",
    "        .replace(\"(\", \"\")\\\n",
    "        .replace(\")\", \"\")\\\n",
    "        .split()\n",
    "    for word in a_string:\n",
    "        build_word = build_word + word + \" \"\n",
    "        if word not in stop_words:\n",
    "            ret_list.append(build_word.strip())\n",
    "            build_word = \"\"\n",
    "    return ret_list\n",
    "\n",
    "training_tokens = tokenize_string(training_raw_text)\n",
    "#print(training_tokens)\n",
    "training_token_ids = [token_to_id[token] for token in training_tokens]\n",
    "#print(\"smol example:\", training_token_ids[:5])\n",
    "print(len(training_token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_length: 17868\n",
      "training_unique_characters_count: 5745\n"
     ]
    }
   ],
   "source": [
    "training_length = len(training_token_ids)\n",
    "unique_tokens = len(tokens)\n",
    "print(\"training_length:\", training_length)\n",
    "print(\"training_unique_characters_count:\", unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the lenght of the past words to take as input\n",
    "input_sequence_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17848"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the input dataset\n",
    "# Example: if input seq length is 3\n",
    "# For text a b c d e f g\n",
    "# gen targets: -> [abc] => d, [bcd] => e, [cde] => f and so on ...\n",
    "# inputs = [abc, bcd, cde]; targets = [d,e,f]\n",
    "# We can call the set of inputs and targets as patterns\n",
    "inputs = []\n",
    "targets = []\n",
    "for i in range(0, training_length - input_sequence_length, 1):\n",
    "    sequence_input = training_tokens[i:i + input_sequence_length]\n",
    "    target = training_tokens[i + input_sequence_length]\n",
    "    inputs.append([token_to_id[token] for token in sequence_input])\n",
    "    targets.append(token_to_id[target])\n",
    "pattern_count = len(inputs)\n",
    "pattern_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE\n",
      "in: [4836, 640, 5349, 1403, 5522, 4943, 3606, 2758, 3296, 603, 934, 2424, 2759, 640, 5332, 1330, 1077, 239, 2715, 870] out 3103\n"
     ]
    }
   ],
   "source": [
    "# see first input and output\n",
    "print(\"EXAMPLE\")\n",
    "print(\"in:\", inputs[0], \"out\", targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# FUN TIME - FORMAT DATA FOR MODEL\n",
    "# --------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsure what 1 is, it says [samples, timesteps, features] in the article,\n",
    "# I guess each character is one feature, if image data this could probs be many? idk?\n",
    "# Dividing by the end with /triaining_unique_characters_count maps inputs to 0-1 range\n",
    "ready_input_data = np.reshape(inputs, (pattern_count, input_sequence_length, 1)) / len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84177546],\n",
       "       [0.11140122],\n",
       "       [0.9310705 ],\n",
       "       [0.24421236],\n",
       "       [0.96118364],\n",
       "       [0.86040035],\n",
       "       [0.62767624],\n",
       "       [0.48006963],\n",
       "       [0.57371628],\n",
       "       [0.10496084]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example prepared input, only showing first 10 of the 100 with [:10]\n",
    "ready_input_data[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_target_data = to_categorical(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17848, 20, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examlpe of ready target data, one hot encoding so of the [0...training_unique_characters_count]\n",
    "# the character is marked as a one where the rest are zeros\n",
    "print(ready_input_data.shape)\n",
    "ready_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# REAL FUN TIME - DEFINE MODEL AAAAND SEEEND IT\n",
    "# ----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_model = Sequential([\n",
    "    LSTM(64, input_shape=(ready_input_data.shape[1], ready_input_data.shape[2])),\n",
    "    Dropout(0.20),\n",
    "    Dense(256, activation='linear'),\n",
    "    Dense(32, activation='linear'),\n",
    "    Dense(ready_target_data.shape[1], activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for training\n",
    "text_generation_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam')\n",
    "# load weights if they exist\n",
    "# if os.path.isfile('text-gen-words-weights.h5'):\n",
    "#     text_generation_model.load_weights('text-gen-words-weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not os.path.isfile('text-gen-words-weights.h5'):\n",
    "    text_generation_model.fit(\n",
    "        ready_input_data,\n",
    "        ready_target_data,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        validation_split=0.1,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_model.save_weights('text-gen-words-weights.h5')  # load weights with model.load_weights(filename)\n",
    "text_generation_model.save('text-gen-words-model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# EVEN MORE REAL FUN TIME - GEN TEXT!!!\n",
    "# ----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: powerful than other peoples skepticism: your own skepticism. you too will judge your early work too harshly. how do you\n",
      "GENERATION: powerful than other peoples skepticism: your own skepticism. you too will judge your early work too harshly. how do you maintenance. maintenance. maintenance. abilities. abilities. abilities. abilities. abilities. heller, abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities. abilities.\n"
     ]
    }
   ],
   "source": [
    "input_id = np.random.randint(0, len(inputs) - 1)\n",
    "seed = inputs[input_id]\n",
    "input_token_ids = seed\n",
    "complete_string = ' '.join([tokens[val] for val in seed])\n",
    "complete_string\n",
    "print(\"SEED:\", complete_string)\n",
    "for i in range(100):\n",
    "    input_sequence = np.reshape(\n",
    "        input_token_ids,\n",
    "        (1, input_sequence_length, 1)\n",
    "    ) / len(tokens)\n",
    "    #print(\"input_sequence\", input_sequence)\n",
    "    output_vector = text_generation_model.predict(input_sequence)    \n",
    "    next_token_id = np.argmax(output_vector)\n",
    "    next_token = tokens[next_token_id]\n",
    "    #print(\"next_token_id\", next_token_id, \"next_token\", next_token)\n",
    "    # append to indices and readable string\n",
    "    #print(\".\", ending='')\n",
    "    input_token_ids.append(next_token_id)\n",
    "    #print(len(input_token_ids))\n",
    "    input_token_ids = input_token_ids[1:]\n",
    "    #print(len(input_token_ids))\n",
    "    complete_string = complete_string + \" \" + next_token\n",
    "print(\"GENERATION:\", complete_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
