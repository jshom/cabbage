{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all the packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all the layer types that we gonna need\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text_filename = \"paul_graham.txt\"\n",
    "training_raw_text = open(\n",
    "    training_text_filename,\n",
    "    'r',\n",
    "    encoding='utf-8')\\\n",
    "    .read()\\\n",
    "    .lower()\\\n",
    "    .replace(\"\\r\",\" \")\\\n",
    "    .replace(\"\\n\",\" \")\\\n",
    "    .replace(\"\\t\",\" \")\\\n",
    "    .replace(\"\\\"\", \"\")\\\n",
    "    .replace(\"\\'\", \"\")\\\n",
    "    .replace(\"(\", \"\")\\\n",
    "    .replace(\")\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the data was actually read correctly\n",
    "training_raw_text = training_raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_string(a_string: str) -> List[str]:\n",
    "    stop_words = open(\"stop_words.txt\", \"r\").read().split('\\n')\n",
    "    ret_list = []\n",
    "    build_word = \"\"\n",
    "    a_string = a_string\\\n",
    "        .lower()\\\n",
    "        .replace(\"\\r\",\" \")\\\n",
    "        .replace(\"\\n\",\" \")\\\n",
    "        .replace(\"\\t\",\" \")\\\n",
    "        .replace(\"\\\"\", \"\")\\\n",
    "        .replace(\"\\'\", \"\")\\\n",
    "        .replace(\"(\", \"\")\\\n",
    "        .replace(\")\", \"\")\\\n",
    "        .split()\n",
    "    for word in a_string:\n",
    "        build_word = build_word + word + \" \"\n",
    "        if word not in stop_words:\n",
    "            ret_list.append(build_word.strip())\n",
    "            build_word = \"\"\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word tokens and mappings from token_id to token and token to token_id\n",
    "tokens = sorted(list(set(tokenize_string(training_raw_text))))\n",
    "token_to_id = dict((token, token_id) for token_id, token in enumerate(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17868\n"
     ]
    }
   ],
   "source": [
    "training_tokens = tokenize_string(training_raw_text)\n",
    "#print(training_tokens)\n",
    "training_token_ids = [token_to_id[token] for token in training_tokens]\n",
    "#print(\"smol example:\", training_token_ids[:5])\n",
    "print(len(training_token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_length: 17868\n",
      "training_unique_characters_count: 5745\n"
     ]
    }
   ],
   "source": [
    "training_length = len(training_tokens)\n",
    "unique_tokens = len(tokens)\n",
    "print(\"training_length:\", training_length)\n",
    "print(\"training_unique_characters_count:\", unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the lenght of the past words to take as input\n",
    "input_sequence_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17858"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the input dataset\n",
    "# Example: if input seq length is 3\n",
    "# For text a b c d e f g\n",
    "# gen targets: -> [abc] => d, [bcd] => e, [cde] => f and so on ...\n",
    "# inputs = [abc, bcd, cde]; targets = [d,e,f]\n",
    "# We can call the set of inputs and targets as patterns\n",
    "inputs = []\n",
    "targets = []\n",
    "for i in range(0, training_length - input_sequence_length, 1):\n",
    "    sequence_input = training_tokens[i:i + input_sequence_length]\n",
    "    target = training_tokens[i + input_sequence_length]\n",
    "    inputs.append([token_to_id[token] for token in sequence_input])\n",
    "    targets.append(token_to_id[target])\n",
    "pattern_count = len(inputs)\n",
    "pattern_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE\n",
      "in: [4836, 640, 5349, 1403, 5522, 4943, 3606, 2758, 3296, 603] out 934\n"
     ]
    }
   ],
   "source": [
    "# see first input and output\n",
    "print(\"EXAMPLE\")\n",
    "print(\"in:\", inputs[0], \"out\", targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# FUN TIME - FORMAT DATA FOR MODEL\n",
    "# --------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsure what 1 is, it says [samples, timesteps, features] in the article,\n",
    "# I guess each character is one feature, if image data this could probs be many? idk?\n",
    "# Dividing by the end with /triaining_unique_characters_count maps inputs to 0-1 range\n",
    "ready_input_data = np.reshape(inputs, (pattern_count, input_sequence_length, 1)) / len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84177546],\n",
       "       [0.11140122],\n",
       "       [0.9310705 ],\n",
       "       [0.24421236],\n",
       "       [0.96118364],\n",
       "       [0.86040035],\n",
       "       [0.62767624],\n",
       "       [0.48006963],\n",
       "       [0.57371628],\n",
       "       [0.10496084]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example prepared input, only showing first 10 of the 100 with [:10]\n",
    "ready_input_data[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_target_data = to_categorical(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17858, 10, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examlpe of ready target data, one hot encoding so of the [0...training_unique_characters_count]\n",
    "# the character is marked as a one where the rest are zeros\n",
    "print(ready_input_data.shape)\n",
    "ready_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# REAL FUN TIME - DEFINE MODEL AAAAND SEEEND IT\n",
    "# ----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_model = Sequential([\n",
    "    LSTM(64, input_shape=(ready_input_data.shape[1], ready_input_data.shape[2])),\n",
    "    Dropout(0.10),\n",
    "    #Dense(256, activation='linear'),\n",
    "    Dense(32, activation='linear'),\n",
    "    Dense(ready_target_data.shape[1], activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for training\n",
    "text_generation_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam')\n",
    "# load weights if they exist\n",
    "# if os.path.isfile('text-gen-words-weights.h5'):\n",
    "#     text_generation_model.load_weights('text-gen-words-weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "503/503 [==============================] - 17s 27ms/step - loss: 8.0106 - val_loss: 8.1427\n",
      "Epoch 2/10\n",
      "503/503 [==============================] - 12s 24ms/step - loss: 7.4476 - val_loss: 8.4086\n",
      "Epoch 3/10\n",
      "503/503 [==============================] - 17s 34ms/step - loss: 7.3511 - val_loss: 8.4941\n",
      "Epoch 4/10\n",
      "503/503 [==============================] - 15s 30ms/step - loss: 7.3544 - val_loss: 8.6934\n",
      "Epoch 5/10\n",
      "503/503 [==============================] - 17s 33ms/step - loss: 7.3527 - val_loss: 8.7699\n",
      "Epoch 6/10\n",
      "503/503 [==============================] - 13s 26ms/step - loss: 7.3620 - val_loss: 8.9762\n",
      "Epoch 7/10\n",
      "503/503 [==============================] - 11s 22ms/step - loss: 7.3384 - val_loss: 9.1189\n",
      "Epoch 8/10\n",
      "503/503 [==============================] - 14s 29ms/step - loss: 7.3128 - val_loss: 9.0068\n",
      "Epoch 9/10\n",
      "503/503 [==============================] - 10s 20ms/step - loss: 7.2974 - val_loss: 9.0560\n",
      "Epoch 10/10\n",
      "503/503 [==============================] - 12s 24ms/step - loss: 7.1967 - val_loss: 9.2255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb1622ab550>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if not os.path.isfile('text-gen-words-weights.h5'):\n",
    "    text_generation_model.fit(\n",
    "        ready_input_data,\n",
    "        ready_target_data,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        validation_split=0.1,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_model.save_weights('text-gen-words-weights.h5')  # load weights with model.load_weights(filename)\n",
    "text_generation_model.save('text-gen-words-model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# EVEN MORE REAL FUN TIME - GEN TEXT!!!\n",
    "# ----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: has a great reputation and theyre clearly working fast to get you\n",
      "GENERATION: has a great reputation and theyre clearly working fast to get you just can there have it if when people they are its you so so it you people when have are it when get youre need as this have your when start is as youre more if this who so dont because more and if to be just to be people get they get when will it when them want raise who get it is not will them but to be its investors or so get is than investors can your when them need or who should more who and it so have it who as this as your have investors this to be but people is more this your will have when what not dont so have or its what if because much but is be and then have its your can to be dont there are to be they need you more if are since than more when it to be you youre start get but is who they if will dont your more not investors and so its dont it people youre because not you you they is youre want want need who want want your want them get its or as they this can people it you is what but but i i they just have and more your to be raise its not there want are is this people investors it are then you be they what should want what should are as then your youre as people who can not are much if but will and have is and youre so dont it if so is there you youre need be how its people i dont i this dont have your they because if who your them money your money if as then not they money its this have your get dont i than its and how are they they want dont as and are are since when dont and to be but what not not not it are is or people more you they are as you people people this youre raise money be is as there is youre it and more be because more dont like youre this want people not do your should they youre need so can much dont to be more people so be there this it do this or can this can we be youre just when youre have dont if who you money get who be and but much to be you is i dont i they are who to be people will but more than but do your they youre you it because its youre your may are as they not it people is be are are then than money they need people youre and when investors so you is who they need they do you it i but your just its there so and are your be to be its dont you its to be what just youre are they i is i because not or youre can be to be youre what but not do there it so they them when not get as will is investors investors more investors because it or are your and so there this and they investors and and than and who what not but not if how they because is will have more it its will as will i not i when when will they so dont and and when as as then can dont investors you you it. you need youre can but not to be just will money as there more are who its you dont not they people it have much have as sometimes but but or if i youre and not if or there they as it because they not but if how like like be your you need will have and not as if are your there to be want you something but much youre it they because if more be you just will not investors is this it people youre to be not they what need they and because but so dont they when they because they because its who money more you will just you get when be be because its it or it are how if your because not i and to be as you just just it is your to be can not there your may more you people will and are much is its your what can can as is they and you are or what investors will money your money because but so is as much will need because so when is i its or you are so and so is as then people your be investors as not and its there as how as if there not so investors more because are will because this have will because have not is youre will if so do than if are not are not it because people if but as this can because then like have your is as will youre to be and be are you just because youre it and if investors people be as to be you they so want want are not to be will if investors you be is so they when should it so get so do if be your when it is this should when it will if you dont investors not is more who need they will to be you is be i this and it to be will just youre money is i to be are because when not can be as your there is and or because its who they be when investors its you get dont not than is and but are there not investors there want raise as because are than when money i people because as or if you and there have as since so but are is more you are are when investors your dont dont much is because i youre when and to be and it how will just so be there it what as is youre as or but much when not and be more are and will because your want raise need be if who can have be will you so you if be if because are if they have youre have be more is it its there be its who get are than if have to be you them it should it to be who it so be when want get your what is dont its to be and this is your want but its and its to be as are if will they are will be because because its so what it you it this can so what them are its as they not this and they and as it because not and than is you so to be to be get so should can its youre i but or it i more and who so want raise is to be when can is this people is because if or i i to be its when investors are more not be are there when dont investors they because they and so be it than you need if not but because then youre to be get you can but i we as how because youre when something have but more who want get more your have your be youre money not or your are they but but not than you is they when are say do much is they as your are because not more are are\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "input_id = np.random.randint(0, len(inputs) - 1)\n",
    "seed = inputs[input_id]\n",
    "input_token_ids = seed\n",
    "complete_string = ' '.join([tokens[val] for val in seed])\n",
    "complete_string\n",
    "print(\"SEED:\", complete_string)\n",
    "for i in range(1200):\n",
    "    input_sequence = np.reshape(\n",
    "        input_token_ids,\n",
    "        (1, input_sequence_length, 1)\n",
    "    ) / len(tokens)\n",
    "    #print(\"input_sequence\", input_sequence)\n",
    "    output_vector = text_generation_model.predict(input_sequence)[0]\n",
    "    proposed_token_ids = []\n",
    "    for i in range(30):\n",
    "        proposed_token_id = np.argmax(output_vector)\n",
    "        output_vector[proposed_token_id] = 0\n",
    "        proposed_token_ids.append(proposed_token_id)\n",
    "    #print(proposed_token_id)\n",
    "    next_token_id = random.choice(proposed_token_ids)\n",
    "    next_token = tokens[next_token_id]\n",
    "    #print(\"next_token_id\", next_token_id, \"next_token\", next_token)\n",
    "    # append to indices and readable string\n",
    "    #print(\".\", ending='')\n",
    "    input_token_ids.append(next_token_id)\n",
    "    #print(len(input_token_ids))\n",
    "    input_token_ids = input_token_ids[1:]\n",
    "    #print(len(input_token_ids))\n",
    "    complete_string = complete_string + \" \" + next_token\n",
    "print(\"GENERATION:\", complete_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
