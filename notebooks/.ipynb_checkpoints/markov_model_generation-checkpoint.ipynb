{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text_filename = \"paul_graham.txt\"\n",
    "training_raw_text = open(\n",
    "    training_text_filename,\n",
    "    'r',\n",
    "    encoding='utf-8')\\\n",
    "    .read()\\\n",
    "    .lower()\n",
    "period_freqs = [len(x.split(\" \")) for x in training_raw_text.split(\".\")]\n",
    "comma_freqs = [len(x.split(\" \")) for x in training_raw_text.split(\",\")]\n",
    "# Alpha-numeric and spaces only\n",
    "# We will re-add punctuation later via normal distributions\n",
    "training_raw_text = re.sub(\"[^a-z0-9'\\\"\\s]\", \"\", training_raw_text)\n",
    "# Remove duplicate sequential space characters\n",
    "training_raw_text = re.sub(\"\\s+\", \" \", training_raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'are', 'two', 'distinct', 'ways', 'to be', 'politically', 'moderate', 'on purpose', 'and', 'by accident', 'intentional', 'moderates', 'are', 'trimmers', 'deliberately', 'choosing', 'a position', 'midway', 'between the extremes']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4934"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_string(input_text):\n",
    "    tokens = []\n",
    "    stop_words = set(open(\"stop_words.txt\", \"r\").read().split('\\n'))\n",
    "    pre_tokenized = input_text.split(\" \")\n",
    "    # Prevents the next chunk from breaking when a stop word ends the data\n",
    "    while pre_tokenized[-1] in stop_words:\n",
    "        pre_tokenized = pre_tokenized[:-1]\n",
    "    for i in range(len(pre_tokenized)):\n",
    "        # Avoids duplication of words when grouping stop words with subsequent ones\n",
    "        if i != 0 and pre_tokenized[i-1] in stop_words:\n",
    "            continue\n",
    "        # Base case, no grouping needed\n",
    "        if pre_tokenized[i] not in stop_words:\n",
    "            tokens.append(pre_tokenized[i])\n",
    "            continue\n",
    "        # Group stop words with subsequent ones until the next word is no longer a stop word\n",
    "        j = i\n",
    "        while pre_tokenized[j] in stop_words:\n",
    "            j += 1\n",
    "        tokens.append(\" \".join(pre_tokenized[i:j+1]))\n",
    "    return tokens\n",
    "tokens = tokenize_string(training_raw_text)\n",
    "print(tokens[:20])\n",
    "len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are -> {'two': 4, 'some kinds': 1, 'almost': 1, 'intellectual': 2, 'some ideas': 2, 'already': 1, 'more': 2, 'lots': 2, 'of course': 1, 'startups': 1, 'lessons': 1, 'now': 1, 'some who': 1, 'rarely': 1, 'no': 2, 'few of those': 1, 'many analogies': 1, 'a handful': 1, 'still': 2, 'certain': 1, 'titles': 1, 'about 40': 1, 'multiple': 1, 'probably': 1, 'only': 1, 'the more': 1}\n"
     ]
    }
   ],
   "source": [
    "markov_model = defaultdict(dict)\n",
    "# Must be sufficiently small enough to prevent over-fitting and large enough to prevent under-fitting\n",
    "# If you see constantly repeated phrases, this value needs to be changed\n",
    "input_window = 2\n",
    "inputs, outputs = [], []\n",
    "for i in range(len(tokens) - input_window):\n",
    "    # Using tuple since they are not mutable\n",
    "    context = tuple(tokens[i:i+input_window])\n",
    "    target = tokens[i+input_window]\n",
    "    if target not in markov_model[context]:\n",
    "        markov_model[context][target] = 0\n",
    "    markov_model[context][target] += 1\n",
    "first_key = list(markov_model.keys())[0]\n",
    "print(f\"{' '.join(first_key)} -> {markov_model[first_key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are -> (('two', 'some kinds', 'almost', 'intellectual', 'some ideas', 'already', 'more', 'lots', 'of course', 'startups', 'lessons', 'now', 'some who', 'rarely', 'no', 'few of those', 'many analogies', 'a handful', 'still', 'certain', 'titles', 'about 40', 'multiple', 'probably', 'only', 'the more'), (0.11428571428571428, 0.02857142857142857, 0.02857142857142857, 0.05714285714285714, 0.05714285714285714, 0.02857142857142857, 0.05714285714285714, 0.05714285714285714, 0.02857142857142857, 0.02857142857142857, 0.02857142857142857, 0.02857142857142857, 0.02857142857142857, 0.02857142857142857, 0.05714285714285714, 0.02857142857142857, 0.02857142857142857, 0.02857142857142857, 0.05714285714285714, 0.02857142857142857, 0.02857142857142857, 0.02857142857142857, 0.02857142857142857, 0.02857142857142857, 0.02857142857142857, 0.02857142857142857))\n"
     ]
    }
   ],
   "source": [
    "# Normalize probabilities and restructure model for prediction efficiency\n",
    "for context, poss in markov_model.items():\n",
    "    total_freq = sum(list(poss.values()))\n",
    "    markov_model[context] = {target: freq / total_freq for target, freq in poss.items()}\n",
    "markov_model = {context: tuple(zip(*list(poss.items()))) for context, poss in markov_model.items()}\n",
    "print(f\"{' '.join(first_key)} -> {markov_model[first_key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED:\n",
      "drafts of this\n",
      "GENERATED (first 20 words, no punctuation):\n",
      "[('one of the biggest',), ('things',), ('holding',), ('people',), ('back',), ('in every',), ('field',), ('but',), ('their',), ('influence',), ('varies',), ('one of the reasons',), ('politics',), ('for example',), ('tends',), ('to be',), ('boring',), ('is',), (\"that it's\",), ('not',)]\n",
      "one of the biggest things holding people back in every field but their influence varies one of the reasons politics\n",
      "0 fully random selections\n"
     ]
    }
   ],
   "source": [
    "seed_idx = np.random.randint(0, len(tokens) - input_window)\n",
    "seed = tuple(tokens[i:i+input_window])\n",
    "print(\"SEED:\")\n",
    "print(\" \".join(seed))\n",
    "print(\"GENERATED (first 20 words, no punctuation):\")\n",
    "full_randoms = 0\n",
    "outputs = []\n",
    "for _ in range(50):\n",
    "    if seed not in markov_model:\n",
    "        idx = np.random.randint(0, len(markov_model))\n",
    "        o = list(markov_model.keys())[idx]\n",
    "        full_randoms += 1\n",
    "    else:\n",
    "        o = tuple(np.random.choice(markov_model[seed][0], 1, p=markov_model[seed][1]).tolist())\n",
    "        # o = (list(markov_model[seed][0])[np.argmax(markov_model[seed][1])],)\n",
    "    if input_window == 1:\n",
    "        seed = tuple(*(list(seed)[1:] + list(o)))\n",
    "    else:\n",
    "        seed = tuple(list(seed)[1:] + list(o))\n",
    "    outputs.append(o)\n",
    "# Splitting and joining here since the punctuation model cares about words not phrases\n",
    "results = (\" \".join([\" \".join(phrase) for phrase in outputs])).split(\" \")\n",
    "print(\" \".join(results[:20]))\n",
    "print(f\"{full_randoms} fully random selections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "one of the biggest things holding people back in every field but their, influence varies one of the reasons politics for example tends to be boring is that it's not that important founders who never explicitly say no some of the worse ones never actually do say no for introductions to other investors that will in many cases be an antirecommendation 8 this is also a good way to tell how serious.\n"
     ]
    }
   ],
   "source": [
    "p_mean, p_std = np.mean(period_freqs), np.std(period_freqs)\n",
    "c_mean, c_std = np.mean(comma_freqs), np.std(comma_freqs)\n",
    "idx = 0\n",
    "stop_words = set(open(\"stop_words.txt\", \"r\").read().split('\\n'))\n",
    "while True:\n",
    "    next_p = np.random.normal(p_mean, p_std, 1).round(0).astype(int)[0]\n",
    "    next_c = np.random.normal(c_mean, c_std, 1).round(0).astype(int)[0]\n",
    "    if next_p + idx >= len(results) or next_c + idx >= len(results):\n",
    "        results[-1] += \".\"\n",
    "        break\n",
    "    # Favoring shorter sentences\n",
    "    symb = \",\"\n",
    "    if next_p <= next_c:\n",
    "        idx += next_p\n",
    "        symb = \".\"\n",
    "    else:\n",
    "        idx += next_c\n",
    "    while results[idx] in stop_words and not idx >= len(results):\n",
    "        idx += 1\n",
    "    results[idx] += symb\n",
    "print(\"Final results:\")\n",
    "print(\" \".join(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
